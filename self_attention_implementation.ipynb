{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Self-attention helps a model look at other words in a sentence to better understand the meaning of each word.\n",
    "\n",
    "Example:\n",
    "\n",
    "\"The animal didn't cross the road because it was too tired.\"\n",
    "\n",
    "To understand \"it\", self-attention lets the model look at other words like \"animal\" and \"road\" to figure out what \"it\" refers to."
   ],
   "id": "75a8be324bbbdab8"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-25T13:02:55.434465Z",
     "start_time": "2025-07-25T13:02:55.430133Z"
    }
   },
   "source": "import numpy as np",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Matrices Generation\n",
    "Making Query, Key and Value matrices and naming them capital as standard representation"
   ],
   "id": "ba2989cc14b43418"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T05:43:12.790873Z",
     "start_time": "2025-07-24T05:43:12.741858Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n, d_k, d_v = 4, 8, 8\n",
    "Q = np.random.randn(n, d_k)\n",
    "K = np.random.randn(n, d_k)\n",
    "V = np.random.randn(n, d_v)"
   ],
   "id": "bb06d25a03093921",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T05:43:13.566718Z",
     "start_time": "2025-07-24T05:43:13.558722Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Query=\",Q)\n",
    "print(\"\\nKey=\",K)\n",
    "print(\"\\nValue=\",V)"
   ],
   "id": "bdf8884809df10d7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query= [[ 1.33378176  0.74266107  1.00125838 -0.03954034 -0.13385055 -0.66041563\n",
      "   1.16017292 -0.11053064]\n",
      " [ 1.95035643  1.07297649  1.26982626 -1.78016591 -1.49168508 -0.06125613\n",
      "   1.27077014 -0.15161438]\n",
      " [ 0.96509567  0.07978951 -0.17827051  0.26079042  0.48183316  0.20851897\n",
      "   0.26478907  1.2802874 ]\n",
      " [ 1.14182796  0.78241199  1.70333113 -0.10560966 -1.57910727 -0.69745217\n",
      "   1.7675734   0.62494639]]\n",
      "\n",
      "Key= [[ 8.54802006e-01  9.36364260e-01 -2.51456675e-01 -1.37107676e+00\n",
      "  -9.92489428e-01 -6.48059844e-02 -6.68484257e-01  1.52679091e-01]\n",
      " [ 6.30565494e-02 -2.15484142e+00 -6.83793696e-01 -7.99846616e-01\n",
      "   9.13496129e-01 -3.34970196e-01  1.99503202e-03  7.27155806e-01]\n",
      " [-4.21160520e-01  8.78936926e-01 -2.20366605e+00 -1.02457266e+00\n",
      "   8.45655345e-01 -2.75215890e+00 -1.89575848e+00 -2.89830405e-01]\n",
      " [-1.57531847e+00  1.00290625e+00  1.36475399e+00  6.52857603e-01\n",
      "   7.32273970e-01  4.98030690e-01  3.75237625e-01  4.63275273e-01]]\n",
      "\n",
      "Value= [[ 0.70414405  0.38402148 -0.48154213 -1.84922261  0.31044938  1.3434345\n",
      "   0.97549298  0.04823276]\n",
      " [-0.02491368 -1.06423817 -0.30441199 -1.0074714  -0.47120786 -1.33916824\n",
      "  -1.03979655  0.77116785]\n",
      " [ 0.1059266  -1.46476611  0.44540455 -1.57368036 -0.41383168 -0.38031377\n",
      "  -0.0059074  -1.36252148]\n",
      " [-0.03601104 -0.66659632 -0.97653689  0.15288441 -0.31292112 -0.46685371\n",
      "   0.68566184  1.55107286]]\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Making Self-Attention Matrix\n",
    "\n",
    "$$\n",
    "\\text{self attention} = softmax\\bigg(\\frac{Q.K^T}{\\sqrt{d_k}}+M\\bigg)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{new V} = \\text{self attention}.V\n",
    "$$\n",
    "\n",
    "So we calculate the Q.K^T matrix and understand why we need to divide the matrix with square-root"
   ],
   "id": "55c01d15e6271c75"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T05:43:16.223648Z",
     "start_time": "2025-07-24T05:43:16.211427Z"
    }
   },
   "cell_type": "code",
   "source": "np.matmul(Q, K.T)",
   "id": "da1f7ee092d60d9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.02117144, -2.14835194, -2.5379063 , -0.05844256],\n",
       "       [ 5.40511651, -3.08339258, -4.31069385, -2.14176006],\n",
       "       [ 0.11367958,  1.10403155, -1.25013519, -0.36418097],\n",
       "       [ 1.95141201, -3.44515251, -6.38647232,  0.69071362]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The reason we need to divide the square-root of d_k, is to normalize the variance. Lets see the difference.",
   "id": "c38503aa65aa6b95"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T05:43:17.591610Z",
     "start_time": "2025-07-24T05:43:17.583751Z"
    }
   },
   "cell_type": "code",
   "source": "Q.var(), K.var(), np.matmul(Q, K.T).var()",
   "id": "c3e813e7926d4a29",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(0.8847350404555572),\n",
       " np.float64(1.1387134946575592),\n",
       " np.float64(7.4351609034128385))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This variance is not normalized and hence must be divided by square-root of d_k to stabilize",
   "id": "ba442fc1bc988aed"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T05:43:18.937413Z",
     "start_time": "2025-07-24T05:43:18.928982Z"
    }
   },
   "cell_type": "code",
   "source": [
    "stabilized = np.matmul(Q, K.T)/np.sqrt(d_k)\n",
    "stabilized.var()"
   ],
   "id": "43543d3fe14673bc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.9293951129266047)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This variance is kind of acceptable",
   "id": "238348970391e96"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Masking\n",
    "\n",
    "This is a prevention from getting context from future word generation. It is used for decoders"
   ],
   "id": "6139b37c557eb85"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T05:43:20.483585Z",
     "start_time": "2025-07-24T05:43:20.475469Z"
    }
   },
   "cell_type": "code",
   "source": [
    "mask = np.tril(np.ones((n,n)))\n",
    "mask"
   ],
   "id": "4804846c13769ccc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [1., 1., 0., 0.],\n",
       "       [1., 1., 1., 0.],\n",
       "       [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Make all the descending words as -ve infinity so the context is lost for decoder. As it wont make sense if decoder has the context from future sentences. Make others zero and add the stabilized matrix to this mask",
   "id": "32ce416402eff28"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T05:43:24.555370Z",
     "start_time": "2025-07-24T05:43:24.547193Z"
    }
   },
   "cell_type": "code",
   "source": [
    "mask[mask==0] = -np.inf\n",
    "mask[mask==1] = 0\n",
    "mask"
   ],
   "id": "b338a7f42bd44011",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0., -inf, -inf, -inf],\n",
       "       [  0.,   0., -inf, -inf],\n",
       "       [  0.,   0.,   0., -inf],\n",
       "       [  0.,   0.,   0.,   0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T05:43:24.767386Z",
     "start_time": "2025-07-24T05:43:24.760382Z"
    }
   },
   "cell_type": "code",
   "source": "stabilized+mask",
   "id": "c27364189b8fbdc4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.36103862,        -inf,        -inf,        -inf],\n",
       "       [ 1.91099727, -1.0901439 ,        -inf,        -inf],\n",
       "       [ 0.0401918 ,  0.3903341 , -0.44198953,        -inf],\n",
       "       [ 0.68992833, -1.21804535, -2.25795894,  0.24420414]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "As we can see, the -ve infinity destroys the context for decoder.",
   "id": "265bf89df8fcbe66"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Softmax operation\n",
    "\n",
    "$$\n",
    "\\text{softmax} = \\frac{e^{x_i}}{\\sum_j e^x_j}\n",
    "$$\n",
    "\n",
    "This is used to transform data into probability distribution so we can get the context."
   ],
   "id": "94a575f9f744991d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T05:43:32.111931Z",
     "start_time": "2025-07-24T05:43:32.094469Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def softmax(x):\n",
    "    return (np.exp(x).T / np.sum(np.exp(x), axis = 1)).T"
   ],
   "id": "6a5e82d25020ce5b",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T05:43:40.423572Z",
     "start_time": "2025-07-24T05:43:40.419305Z"
    }
   },
   "cell_type": "code",
   "source": "attention = softmax(stabilized+mask)",
   "id": "6cd97c1bf9984024",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T05:43:41.728250Z",
     "start_time": "2025-07-24T05:43:41.722113Z"
    }
   },
   "cell_type": "code",
   "source": "attention",
   "id": "83c651ec09cf3136",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        , 0.        , 0.        ],\n",
       "       [0.95262565, 0.04737435, 0.        , 0.        ],\n",
       "       [0.32930434, 0.4673716 , 0.20332405, 0.        ],\n",
       "       [0.54312655, 0.08058952, 0.02848721, 0.34779672]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Getting the new value matrix with multiplying the V matrix with attention matrix",
   "id": "c41de5fa26733e5e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T05:43:50.175684Z",
     "start_time": "2025-07-24T05:43:50.168682Z"
    }
   },
   "cell_type": "code",
   "source": [
    "newV = np.matmul(attention, V)\n",
    "newV  # After Attention"
   ],
   "id": "de132ea6d89fee06",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.70414405,  0.38402148, -0.48154213, -1.84922261,  0.31044938,\n",
       "         1.3434345 ,  0.97549298,  0.04823276],\n",
       "       [ 0.66960542,  0.31541113, -0.47315071, -1.8093452 ,  0.27341888,\n",
       "         1.21634795,  0.88001996,  0.08248133],\n",
       "       [ 0.24177117, -0.66875695, -0.21028598, -1.39978763, -0.20213878,\n",
       "        -0.26081733, -0.16593842,  0.09927182],\n",
       "       [ 0.37092458, -0.15076129, -0.61301873, -1.07721061,  0.01001704,\n",
       "         0.44852775,  0.68432208,  0.58898816]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T05:43:50.985951Z",
     "start_time": "2025-07-24T05:43:50.980513Z"
    }
   },
   "cell_type": "code",
   "source": "V # Before attention",
   "id": "ab8c3bae12207999",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.70414405,  0.38402148, -0.48154213, -1.84922261,  0.31044938,\n",
       "         1.3434345 ,  0.97549298,  0.04823276],\n",
       "       [-0.02491368, -1.06423817, -0.30441199, -1.0074714 , -0.47120786,\n",
       "        -1.33916824, -1.03979655,  0.77116785],\n",
       "       [ 0.1059266 , -1.46476611,  0.44540455, -1.57368036, -0.41383168,\n",
       "        -0.38031377, -0.0059074 , -1.36252148],\n",
       "       [-0.03601104, -0.66659632, -0.97653689,  0.15288441, -0.31292112,\n",
       "        -0.46685371,  0.68566184,  1.55107286]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Self Attention function\n",
    "\n",
    "Implementing all the steps we discussed in a single function"
   ],
   "id": "1a10bc51c87d7f7e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T05:45:32.763467Z",
     "start_time": "2025-07-24T05:45:32.757107Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def self_attention(Q, K, V, mask = None):\n",
    "    d_k = Q.shape[-1]\n",
    "    stable = np.matmul(Q, K.T) / np.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        stable = stable+mask\n",
    "    attention = softmax(stable)\n",
    "    Output = np.matmul(attention, V)\n",
    "    return Output"
   ],
   "id": "aca01eb9a914a44",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T05:45:33.510218Z",
     "start_time": "2025-07-24T05:45:33.505223Z"
    }
   },
   "cell_type": "code",
   "source": [
    "Output = self_attention(Q, K, V)\n",
    "print(Output)"
   ],
   "id": "134c054325bfc7a7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.3059566  -0.36385116 -0.48886891 -1.09925603 -0.07606805  0.20932842\n",
      "   0.48098292  0.42368889]\n",
      " [ 0.61131096  0.20641473 -0.47777447 -1.68451183  0.21885309  1.07023614\n",
      "   0.84351312  0.13053094]\n",
      " [ 0.18132645 -0.6682868  -0.37702028 -1.06193003 -0.22624474 -0.30565034\n",
      "   0.01936769  0.41518013]\n",
      " [ 0.37092458 -0.15076129 -0.61301873 -1.07721061  0.01001704  0.44852775\n",
      "   0.68432208  0.58898816]]\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This output is the contextualized version due to self-attention algorithm, with masked future context in every array so as the decoder doesn't use the future context to understand past. The steps are clear and straightforward with explaination of the contextual need for decoding.",
   "id": "52900a2b45e4a9e9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "ü§π‚Äç‚ôÇÔ∏è Why Multi-Head Attention Instead of Just One Self-Attention?\n",
    "Imagine you have just one spotlight (a single head) ‚Äî it can focus on one type of relationship.\n",
    "\n",
    "But different heads can focus on different things at the same time!\n",
    "\n",
    "üîç Here's the idea:\n",
    "One head might focus on nearby words\n",
    "\n",
    "Another on important verbs\n",
    "\n",
    "Another on grammatical structure\n",
    "\n",
    "Another on long-range dependencies\n",
    "\n",
    "So instead of just one way of looking at the sentence, multi-head attention gives the model multiple perspectives."
   ],
   "id": "ee471847cd2b654d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Multi Head Attention\n",
    "\n",
    "We will walk through the whole process of making Q, V and K vector through the input. I'll be using PyTorch with numpy for the ease of using Linear Models and Neural Networks.\n",
    "\n",
    "`Note`: This all can be done by numpy as done above, but using the tools to keep things up to date."
   ],
   "id": "b41114627843b39a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T12:44:30.534107Z",
     "start_time": "2025-07-25T12:44:27.232228Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ],
   "id": "a8f02a09e633b408",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Taking an example case for code",
   "id": "36274d62940cf959"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T12:44:30.581158Z",
     "start_time": "2025-07-25T12:44:30.559723Z"
    }
   },
   "cell_type": "code",
   "source": [
    "seq_len = 4 # Length of input\n",
    "batch_size = 1 # for construction of architecture, we can increase it after implementing the function\n",
    "input_dim = 512 # Dimensions of input\n",
    "d_model = 512 # Output of the attention unit for every single word\n",
    "X = torch.randn(batch_size, seq_len, input_dim)   # Can use Numpy's random function\n",
    "X.size()"
   ],
   "id": "db0d207023283ba2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 512])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now we map the input to 3x the output dim, as to make Q, K, V vectors concatenated, which will be split in next steps",
   "id": "1ba0a334d4473a8a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T12:44:30.631654Z",
     "start_time": "2025-07-25T12:44:30.610167Z"
    }
   },
   "cell_type": "code",
   "source": "QKV_layer = nn.Linear(input_dim, 3*d_model)",
   "id": "9bc05b65c772d68e",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T12:44:30.670661Z",
     "start_time": "2025-07-25T12:44:30.641663Z"
    }
   },
   "cell_type": "code",
   "source": [
    "QKV = QKV_layer(X)\n",
    "QKV.size()"
   ],
   "id": "5e3f32859e2959a6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 1536])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Lets see the normal distribution data we have generated through the randn function",
   "id": "60533b96b46c4832"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T13:02:59.405263Z",
     "start_time": "2025-07-25T13:02:58.854404Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x = np.arange(-1,1,0.01)*3\n",
    "y = torch.histc(QKV, bins=200, min=-3, max=3)\n",
    "plt.bar(x,y,align='center',color=['blue'])\n",
    "plt.title(\"QKV distribution\")"
   ],
   "id": "c2a8dfaa1ae9185e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'QKV distribution')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAAGzCAYAAAAFROyYAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjMsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvZiW1igAAAAlwSFlzAAAPYQAAD2EBqD+naQAALslJREFUeJzt3X9cVHW+x/H3IDCQyiCoICsg2Q+1H1r+IMrNLAo1NUsri4ySslqwq9gP2cdVc/tBv/Pqala3q9VmttWiq7treTGlNiTFNcuS0kVFvYAbMSOUiHLuHz2cdgQVdMb5Aq/n43H+mO855zufOflg3n3nfL/HZlmWJQAAAIME+LsAAACAYxFQAACAcQgoAADAOAQUAABgHAIKAAAwDgEFAAAYh4ACAACMQ0ABAADGIaAAAADjEFAAeIXNZtNjjz3mfr148WLZbDbt3LnT5+991113qUePHu7XO3fulM1m0/PPP+/z95akxx57TDab7Yy8F9BWEFAAA23dulV33HGHfvWrX8lutysmJkZ33HGHvv766wbHHg0CGzdu9Gh3Op0aNGiQQkJCtGrVKl188cWKi4vTiZ5uccUVVygqKkqHDx/2+mdqih9//FGPPfaY1q5d65f3PxGTawNaIwIKYJg//elPuvTSS5WXl6e7775bCxYsUHp6utasWaNLL71Uy5cvP2kfLpdL1113nbZs2aLc3FwNGzZMqampKi0t1SeffNLoOTt37lRBQYFuvfVWBQYGnvbnmDBhgn766SfFx8c3+Zwff/xRs2fPbnYIeO2111RcXNzMCpvnRLX953/+p3766Sefvj/Q1hBQAIPs2LFDEyZM0Nlnn60tW7boiSeeUHp6uh5//HFt2bJFCQkJuuOOO1RSUnLcPg4cOKCUlBRt3rxZH3zwgYYPHy5Juv3222Wz2bRkyZJGz3vnnXdkWZZSU1O98lnatWunkJAQn/70UVNTI0kKCgqS3W732fucTGBgoEJCQvz2/kBrREABDPLcc8/pxx9/1KuvvqouXbp47OvcubNeeeUVVVdX67nnnmv0/Orqag0bNkybNm3SBx98oOuvv969LzY2VldeeaXef/991dXVNTh3yZIl6tmzpxITE09YY21traZOnaouXbqoY8eOGj16tPbs2dPguMbuQdm4caNSUlLUuXNnhYaGKiEhQRMnTpT08wjO0c88e/Zs2Ww2j/ta7rrrLnXo0EE7duzQiBEj1LFjR3eYOvYelH/30ksvKT4+XqGhoRoyZIi++uorj/1XXXWVrrrqqgbn/XufJ6utsXtQDh8+rMcff1w9e/aU3W5Xjx499Nvf/la1tbUex/Xo0UMjR47Up59+6v5J7uyzz9abb77Z6OcB2goCCmCQFStWqEePHvr1r3/d6P4rr7xSPXr00IoVKxrsq6mp0fDhw7Vhwwa99957GjlyZINjUlNT9f333+vDDz/0aP/yyy/11VdfNWn05J577tGcOXN03XXX6emnn1ZQUJBHEDqeiooKXXfdddq5c6emT5+uefPmKTU1VevXr5ckdenSRS+//LIk6cYbb9Rbb72lt956SzfddJO7j8OHDyslJUVdu3bV888/r7Fjx57wPd98803NnTtXGRkZys7O1ldffaWrr75a5eXlJ6333zWltmPdc889mjlzpi699FK99NJLGjJkiHJycjR+/PgGx27fvl3jxo3TtddeqxdeeEGdOnXSXXfdpa1btzarTqBVsQAYoaqqypJk3XDDDSc8bvTo0ZYky+VyWZZlWYsWLbIkWfHx8VZQUJC1bNmy455bWVlp2e1267bbbvNonz59uiXJKi4uPuF7b9682ZJk/eY3v/Fov/322y1J1qxZs9xtR+sqKSmxLMuycnNzLUnWhg0bjtv//v37G/RzVFpamiXJmj59eqP74uPj3a9LSkosSVZoaKi1Z88ed3thYaElyZo6daq7bciQIdaQIUNO2ueJaps1a5b1739Oj16ne+65x+O4hx56yJJkrVmzxt0WHx9vSbLy8/PdbRUVFZbdbremTZvW4L2AtoIRFMAQBw4ckCR17NjxhMcd3X/0+KPKy8sVEhKi2NjY457bqVMnjRgxQn/+85/d929YlqWlS5dqwIABOu+880743n/9618lSQ8++KBH+5QpU054niSFh4dLklauXNnoT0xN9cADDzT52DFjxuhXv/qV+/WgQYOUmJjo/hy+crT/rKwsj/Zp06ZJkv7yl794tPfp08dj1KxLly46//zz9c9//tOndQImI6AAhjhe8DjWgQMHZLPZ1LlzZ4/2V155RcHBwRo2bNgJZ7SkpqaqpqbGPRvos88+086dO5v0886uXbsUEBCgnj17erSff/75Jz13yJAhGjt2rGbPnq3OnTvrhhtu0KJFixrck3EigYGB6t69e5OPP/fccxu0nXfeeT5fm+XodTrnnHM82qOjoxUeHq5du3Z5tMfFxTXoo1OnTvrhhx98WidgMgIKYAiHw6GYmBht2bLlhMdt2bJF3bt3V3BwsEd7nz599Ne//lU//fSTrr32WpWWljZ6/siRI+VwONyzeZYsWaJ27do1em+EN9lsNr3//vsqKChQZmam9u7dq4kTJ6p///6qrq5uUh92u10BAd79s3W8WUZHjhzxWd/HateuXaPt1gnWrAFaOwIKYJBRo0appKREn376aaP7P/nkE+3cuVM333xzo/sHDRqkZcuWqaKiQtdee63279/f4Bi73a5x48bpo48+Unl5ud577z1dffXVio6OPml98fHxqq+v144dOzzam7MGyWWXXaYnn3xSGzdu1Ntvv62tW7dq6dKlkpr+hd5U3333XYO2b7/91mPGT6dOnVRVVdXguGNHOZpT29HrdOz7l5eXq6qqqllrwwBtFQEFMMhDDz2ks846S/fdd5++//57j32VlZW6//77FRYWpszMzOP2cc011+idd97R9u3bNWzYMLlcrgbHpKamqq6uTvfdd5/279/f5LVPjq6pMnfuXI/2OXPmnPTcH374ocGIQL9+/STJ/TPPWWedJUmNBoZTsWzZMu3du9f9+vPPP1dhYaH7c0hSz549tW3bNo8w98UXX+jvf/+7R1/NqW3EiBGSGl6XF198UZKaNOsJaOtOf7lIAF5zzjnn6M0339Rtt92miy66SOnp6UpISNDOnTv1+uuv64cfftDSpUuVkJBwwn5uvPFGvfbaa5o4caJGjx6tVatWeSwkNmTIEHXv3l3Lly9XaGjoCafL/rt+/frptttu04IFC+R0OnX55ZcrLy9P27dvP+m5b7zxhhYsWKAbb7xRPXv21IEDB/Taa68pLCzM/YUeGhqqPn366N1339V5552niIgIXXjhhbrwwgubVN+xzjnnHA0ePFgPPPCAamtrNWfOHEVGRuqRRx5xHzNx4kS9+OKLSklJUXp6uioqKrRw4UJdcMEFHuGuObX17dtXaWlpevXVV1VVVaUhQ4bo888/1xtvvKExY8Zo6NChp/R5gDbFz7OIADTiyy+/tG6//XYrOjraCggIsCRZISEh1tatWxsce3Q6b2PTd59//nlLkjVy5Eirrq7OY9/DDz9sSbJuueWWZtX2008/WQ8++KAVGRlptW/f3ho1apRVWlp60mnGmzZtsm677TYrLi7OstvtVteuXa2RI0daGzdu9Oj/s88+s/r3728FBwd79JmWlma1b9++0ZqON834ueees1544QUrNjbWstvt1q9//Wvriy++aHD+H/7wB+vss8+2goODrX79+lkffvhhgz5PVNux04wty7Lq6uqs2bNnWwkJCVZQUJAVGxtrZWdnWwcPHvQ4Lj4+3rr++usb1HS86c9AW2GzLO7CAkz35ptv6q677tIdd9zBCqMA2gR+4gFagDvvvFP/93//p+nTp6t79+566qmn/F0SAPgUIygAAMA4zOIBAADGIaAAAADjEFAAAIBxCCgAAMA4LXIWT319vfbt26eOHTt6fWlsAADgG5Zl6cCBA4qJiTnpc7VaZEDZt2/fCR8pDwAAzFVaWnrSJ5O3yIBy9LH0paWlCgsL83M1AACgKVwul2JjY93f4yfSIgPK0Z91wsLCCCgAALQwTbk9g5tkAQCAcQgoAADAOAQUAABgHAIKAAAwDgEFAAAYh4ACAACMQ0ABAADGIaAAAADjEFAAAIBxCCgAAMA4BBQAAGAcAgoAADAOAQUAABiHgAIAAIwT6O8CAJipCU9Db3Esy98VAGiqZo+g5Ofna9SoUYqJiZHNZtOyZcsaHPPNN99o9OjRcjgcat++vQYOHKjdu3e79x88eFAZGRmKjIxUhw4dNHbsWJWXl5/WBwEAAK1HswNKTU2N+vbtq/nz5ze6f8eOHRo8eLB69eqltWvXasuWLZoxY4ZCQkLcx0ydOlUrVqzQe++9p3Xr1mnfvn266aabTv1TAACAVsVmWac+6Gmz2ZSbm6sxY8a428aPH6+goCC99dZbjZ7jdDrVpUsXLVmyROPGjZMkbdu2Tb1791ZBQYEuu+yyk76vy+WSw+GQ0+lUWFjYqZYP4AT4iQeAtzXn+9urN8nW19frL3/5i8477zylpKSoa9euSkxM9PgZqKioSHV1dUpOTna39erVS3FxcSooKGi039raWrlcLo8NAAC0Xl4NKBUVFaqurtbTTz+tYcOG6aOPPtKNN96om266SevWrZMklZWVKTg4WOHh4R7nRkVFqaysrNF+c3Jy5HA43FtsbKw3ywYAAIbx+giKJN1www2aOnWq+vXrp+nTp2vkyJFauHDhKfebnZ0tp9Pp3kpLS71VMgAAMJBXpxl37txZgYGB6tOnj0d779699emnn0qSoqOjdejQIVVVVXmMopSXlys6OrrRfu12u+x2uzdLBQAABvPqCEpwcLAGDhyo4uJij/Zvv/1W8fHxkqT+/fsrKChIeXl57v3FxcXavXu3kpKSvFkOAABooZo9glJdXa3t27e7X5eUlGjz5s2KiIhQXFycHn74Yd1666268sorNXToUK1atUorVqzQ2rVrJUkOh0Pp6enKyspSRESEwsLCNHnyZCUlJTVpBg8AAGj9mj3NeO3atRo6dGiD9rS0NC1evFiS9D//8z/KycnRnj17dP7552v27Nm64YYb3McePHhQ06ZN0zvvvKPa2lqlpKRowYIFx/2J51hMMwZ8j2nGALytOd/fp7UOir8QUADfI6AA8Da/rYMCAADgDQQUAABgHAIKAAAwDgEFAAAYh4ACAACMQ0ABAADGIaAAAADjEFAAAIBxCCgAAMA4BBQAAGAcAgoAADAOAQUAABiHgAIAAIxDQAEAAMYhoAAAAOMQUAAAgHEIKAAAwDiB/i4AAM4Um83ztWX5pw4AJ8cICgAAMA4BBQAAGIeAAgAAjENAAQAAxiGgAAAA4xBQAACAcQgoAADAOAQUAABgHAIKAAAwDgEFAAAYh4ACAACMQ0ABAADGIaAAAADjEFAAAIBxmh1Q8vPzNWrUKMXExMhms2nZsmXHPfb++++XzWbTnDlzPNorKyuVmpqqsLAwhYeHKz09XdXV1c0tBQAAtFLNDig1NTXq27ev5s+ff8LjcnNztX79esXExDTYl5qaqq1bt2r16tVauXKl8vPzNWnSpOaWAgAAWqnA5p4wfPhwDR8+/ITH7N27V5MnT9aHH36o66+/3mPfN998o1WrVmnDhg0aMGCAJGnevHkaMWKEnn/++UYDDQAAaFu8fg9KfX29JkyYoIcfflgXXHBBg/0FBQUKDw93hxNJSk5OVkBAgAoLCxvts7a2Vi6Xy2MDAACtl9cDyjPPPKPAwEA9+OCDje4vKytT165dPdoCAwMVERGhsrKyRs/JycmRw+Fwb7Gxsd4uGwAAGMSrAaWoqEj/9V//pcWLF8tms3mt3+zsbDmdTvdWWlrqtb4BAIB5vBpQPvnkE1VUVCguLk6BgYEKDAzUrl27NG3aNPXo0UOSFB0drYqKCo/zDh8+rMrKSkVHRzfar91uV1hYmMcGwDdstp83APCnZt8keyITJkxQcnKyR1tKSoomTJigu+++W5KUlJSkqqoqFRUVqX///pKkNWvWqL6+XomJid4sBwAAtFDNDijV1dXavn27+3VJSYk2b96siIgIxcXFKTIy0uP4oKAgRUdH6/zzz5ck9e7dW8OGDdO9996rhQsXqq6uTpmZmRo/fjwzeAAAgKRT+Iln48aNuuSSS3TJJZdIkrKysnTJJZdo5syZTe7j7bffVq9evXTNNddoxIgRGjx4sF599dXmlgIAAFopm2VZlr+LaC6XyyWHwyGn08n9KICXtaX7T1reXz+gZWvO9zfP4gEAAMYhoAAAAOMQUAAAgHEIKAAAwDgEFAAAYBwCCgAAMA4BBQAAGIeAAgAAjENAAQAAxiGgAAAA4xBQAACAcQgoAADAOAQUAABgHAIKAAAwDgEFAAAYJ9DfBQCAv9hszTvesnxTB4CGGEEBAADGIaAAAADjEFAAAIBxCCgAAMA4BBQAAGAcAgoAADAOAQUAABiHgAIAAIxDQAEAAMYhoAAAAOMQUAAAgHEIKAAAwDg8LBAAmuhkDxfkYYKA9zCCAgAAjMMICtDGnWxUAAD8gREUAABgHAIKAAAwTrMDSn5+vkaNGqWYmBjZbDYtW7bMva+urk6PPvqoLrroIrVv314xMTG68847tW/fPo8+KisrlZqaqrCwMIWHhys9PV3V1dWn/WEAAEDr0OyAUlNTo759+2r+/PkN9v3444/atGmTZsyYoU2bNulPf/qTiouLNXr0aI/jUlNTtXXrVq1evVorV65Ufn6+Jk2adOqfAgAAtCo2yzr1iXE2m025ubkaM2bMcY/ZsGGDBg0apF27dikuLk7ffPON+vTpow0bNmjAgAGSpFWrVmnEiBHas2ePYmJiTvq+LpdLDodDTqdTYWFhp1o+AHGTrDcxzRg4seZ8f/v8HhSn0ymbzabw8HBJUkFBgcLDw93hRJKSk5MVEBCgwsLCRvuora2Vy+Xy2AAAQOvl04By8OBBPfroo7rtttvcSamsrExdu3b1OC4wMFAREREqKytrtJ+cnBw5HA73Fhsb68uygVbBZmvaBgAm8llAqaur0y233CLLsvTyyy+fVl/Z2dlyOp3urbS01EtVAgAAE/lkobaj4WTXrl1as2aNx+9M0dHRqqio8Dj+8OHDqqysVHR0dKP92e122e12X5QKAAAM5PURlKPh5LvvvtP//u//KjIy0mN/UlKSqqqqVFRU5G5bs2aN6uvrlZiY6O1yAABAC9TsEZTq6mpt377d/bqkpESbN29WRESEunXrpnHjxmnTpk1auXKljhw54r6vJCIiQsHBwerdu7eGDRume++9VwsXLlRdXZ0yMzM1fvz4Js3gAQAArV+zpxmvXbtWQ4cObdCelpamxx57TAkJCY2e9/HHH+uqq66S9PNCbZmZmVqxYoUCAgI0duxYzZ07Vx06dGhSDUwzBk6OG2DPPKYZAyfWnO/v01oHxV8IKMDxEUz8p+X9NQXOLKPWQQEAAGguAgoAeAlrywDeQ0ABAADGIaAAAADjEFAAAIBxCCgAAMA4BBQAAGAcAgoAADCOTx4WCABt2bFTjVnADWg+RlAAAIBxCCgAAMA4BBQAAGAc7kEBWjiWVgfQGjGCAgAAjENAAQAAxiGgAAAA4xBQAACAcQgoAADAOAQUAABgHAIKAPiYzcZ0cKC5CCgAAMA4LNQGtFD8HzmA1owRFAAAYBwCCgAAMA4BBQAAGIeAAgAAjENAAQAAxiGgAAAA4xBQAACAcQgoAADAOAQUAABgHAIKAAAwTrMDSn5+vkaNGqWYmBjZbDYtW7bMY79lWZo5c6a6deum0NBQJScn67vvvvM4prKyUqmpqQoLC1N4eLjS09NVXV19Wh8EAAC0Hs0OKDU1Nerbt6/mz5/f6P5nn31Wc+fO1cKFC1VYWKj27dsrJSVFBw8edB+TmpqqrVu3avXq1Vq5cqXy8/M1adKkU/8UAACgVbFZlmWd8sk2m3JzczVmzBhJP4+exMTEaNq0aXrooYckSU6nU1FRUVq8eLHGjx+vb775Rn369NGGDRs0YMAASdKqVas0YsQI7dmzRzExMSd9X5fLJYfDIafTqbCwsFMtH2hReDhgy3fqf22B1qE5399evQelpKREZWVlSk5Odrc5HA4lJiaqoKBAklRQUKDw8HB3OJGk5ORkBQQEqLCwsNF+a2tr5XK5PDYAANB6eTWglJWVSZKioqI82qOiotz7ysrK1LVrV4/9gYGBioiIcB9zrJycHDkcDvcWGxvrzbIB4Iyw2RgJA5qqRcziyc7OltPpdG+lpaX+LgkAAPiQVwNKdHS0JKm8vNyjvby83L0vOjpaFRUVHvsPHz6syspK9zHHstvtCgsL89gAAEDr5dWAkpCQoOjoaOXl5bnbXC6XCgsLlZSUJElKSkpSVVWVioqK3MesWbNG9fX1SkxM9GY5AACghQps7gnV1dXavn27+3VJSYk2b96siIgIxcXFacqUKXriiSd07rnnKiEhQTNmzFBMTIx7pk/v3r01bNgw3XvvvVq4cKHq6uqUmZmp8ePHN2kGDwC0dEfvQ2FWD3B8zQ4oGzdu1NChQ92vs7KyJElpaWlavHixHnnkEdXU1GjSpEmqqqrS4MGDtWrVKoWEhLjPefvtt5WZmalrrrlGAQEBGjt2rObOneuFjwMAAFqD01oHxV9YBwVtEbM/Wp+W99cXOD1+WwcFAADAGwgoAADAOAQUAABgHAIKAAAwDgEFAAAYh4ACAACMQ0ABAADGIaAAAADjEFAAAIBxCCgAAMA4BBQAAGAcAgoAADAOAQUAABiHgAIAfmKz8ZRq4HgIKAAAwDgEFAAAYBwCCgAAMA4BBQAAGIeAAgAAjENAAQAAxiGgAAAA4xBQAACAcQgoAADAOIH+LgAA2rpjV5O1LP/UAZiEERQAAGAcAgoAADAOAQUAABiHgAIAAIxDQAEAAMYhoAAAAOMQUAAAgHEIKAAAwDgs1AYY7thFvACgLfD6CMqRI0c0Y8YMJSQkKDQ0VD179tTjjz8u69+WRrQsSzNnzlS3bt0UGhqq5ORkfffdd94uBQAAtFBeDyjPPPOMXn75Zf3+97/XN998o2eeeUbPPvus5s2b5z7m2Wef1dy5c7Vw4UIVFhaqffv2SklJ0cGDB71dDgAAaIFsluXdpz6MHDlSUVFRev31191tY8eOVWhoqP7whz/IsizFxMRo2rRpeuihhyRJTqdTUVFRWrx4scaPH9+gz9raWtXW1rpfu1wuxcbGyul0KiwszJvlA8bhJ562h2fxoLVyuVxyOBxN+v72+gjK5Zdfrry8PH377beSpC+++EKffvqphg8fLkkqKSlRWVmZkpOT3ec4HA4lJiaqoKCg0T5zcnLkcDjcW2xsrLfLBgAABvH6TbLTp0+Xy+VSr1691K5dOx05ckRPPvmkUlNTJUllZWWSpKioKI/zoqKi3PuOlZ2draysLPfroyMoAACgdfJ6QPnjH/+ot99+W0uWLNEFF1ygzZs3a8qUKYqJiVFaWtop9Wm322W3271cKQAAMJXXA8rDDz+s6dOnu+8lueiii7Rr1y7l5OQoLS1N0dHRkqTy8nJ169bNfV55ebn69evn7XIAAEAL5PV7UH788UcFBHh2265dO9XX10uSEhISFB0drby8PPd+l8ulwsJCJSUlebscAADQAnl9BGXUqFF68sknFRcXpwsuuED/+Mc/9OKLL2rixImSJJvNpilTpuiJJ57Queeeq4SEBM2YMUMxMTEaM2aMt8sBjMXsHAA4Pq8HlHnz5mnGjBn6zW9+o4qKCsXExOi+++7TzJkz3cc88sgjqqmp0aRJk1RVVaXBgwdr1apVCgkJ8XY5AACgBfL6OihnQnPmUQOmYgQFx9Py/ioDTePXdVAAAABOFwEFAAAYh4ACAACMQ0ABAADGIaAAAADjEFAAAIBxCCgAAMA4BBQAAGAcAgoAGMZmYyE/gIACAACMQ0ABAADGIaAAAADjEFAAAIBxCCgAAMA4BBQAAGAcAgoAADAOAQUADMV6KGjLCCgAAMA4BBQAAGAcAgoAADAOAQUAABiHgAIAAIxDQAEAAMYhoAAAAOMQUAAAgHEIKAAAwDgEFAAAYBwCCgAAMA4BBQAAGIeAAgAAjENAAQAAxiGgAAAA4/gkoOzdu1d33HGHIiMjFRoaqosuukgbN25077csSzNnzlS3bt0UGhqq5ORkfffdd74oBTCOzfbzBjQV/2bQFnk9oPzwww+64oorFBQUpL/97W/6+uuv9cILL6hTp07uY5599lnNnTtXCxcuVGFhodq3b6+UlBQdPHjQ2+UAAIAWyGZZluXNDqdPn66///3v+uSTTxrdb1mWYmJiNG3aND300EOSJKfTqaioKC1evFjjx48/6Xu4XC45HA45nU6FhYV5s3zA5/g/YZwq7/61Bs685nx/e30E5c9//rMGDBigm2++WV27dtUll1yi1157zb2/pKREZWVlSk5Odrc5HA4lJiaqoKCg0T5ra2vlcrk8NgAA0Hp5PaD885//1Msvv6xzzz1XH374oR544AE9+OCDeuONNyRJZWVlkqSoqCiP86Kiotz7jpWTkyOHw+HeYmNjvV02AAAwiNcDSn19vS699FI99dRTuuSSSzRp0iTde++9Wrhw4Sn3mZ2dLafT6d5KS0u9WDEAADCN1wNKt27d1KdPH4+23r17a/fu3ZKk6OhoSVJ5ebnHMeXl5e59x7Lb7QoLC/PYAKCtYTYP2hKvB5QrrrhCxcXFHm3ffvut4uPjJUkJCQmKjo5WXl6ee7/L5VJhYaGSkpK8XQ4AAGiBAr3d4dSpU3X55Zfrqaee0i233KLPP/9cr776ql599VVJks1m05QpU/TEE0/o3HPPVUJCgmbMmKGYmBiNGTPG2+UAAIAWyOsBZeDAgcrNzVV2drZ+97vfKSEhQXPmzFFqaqr7mEceeUQ1NTWaNGmSqqqqNHjwYK1atUohISHeLgcAALRAXl8H5UxgHRS0ZNxDgNPV8v5qAz/z6zooAAAAp4uAAgAAjOP1e1AAAL517M+E/OSD1ogRFAAAYBxGUAAv4yZYnGlH/80xkoLWhBEUAABgHAIKAAAwDgEFAAAYh4ACAACMQ0ABAADGIaAAAADjEFAAAIBxCCgA0ErYbKzDg9aDgAIAAIxDQAEAAMZhqXvASxhaBwDvYQQFAAAYh4ACAACMQ0ABAADGIaAAAADjEFAAAIBxCCgAAMA4BBQAAGAcAgoAADAOAQUAABiHgAIAAIxDQAEAAMYhoAAAAOMQUACglbHZeHglWj4CCgAAMA4BBQAAGIeAAgAAjENAAQAAxvF5QHn66adls9k0ZcoUd9vBgweVkZGhyMhIdejQQWPHjlV5ebmvSwEAAC2ETwPKhg0b9Morr+jiiy/2aJ86dapWrFih9957T+vWrdO+fft00003+bIUwGeYMQEA3uezgFJdXa3U1FS99tpr6tSpk7vd6XTq9ddf14svvqirr75a/fv316JFi/TZZ59p/fr1jfZVW1srl8vlsQEAgNbLZwElIyND119/vZKTkz3ai4qKVFdX59Heq1cvxcXFqaCgoNG+cnJy5HA43FtsbKyvygaAVoPRPbRkPgkoS5cu1aZNm5STk9NgX1lZmYKDgxUeHu7RHhUVpbKyskb7y87OltPpdG+lpaW+KBsAABgi0NsdlpaW6j/+4z+0evVqhYSEeKVPu90uu93ulb4AAID5vD6CUlRUpIqKCl166aUKDAxUYGCg1q1bp7lz5yowMFBRUVE6dOiQqqqqPM4rLy9XdHS0t8sBAAAtkNdHUK655hp9+eWXHm133323evXqpUcffVSxsbEKCgpSXl6exo4dK0kqLi7W7t27lZSU5O1yAABAC+T1gNKxY0ddeOGFHm3t27dXZGSkuz09PV1ZWVmKiIhQWFiYJk+erKSkJF122WXeLgcAALRAXg8oTfHSSy8pICBAY8eOVW1trVJSUrRgwQJ/lAIAAAxksyzL8ncRzeVyueRwOOR0OhUWFubvctDGMY0Tpmt5f+XRWjXn+5tn8QAAAOMQUAAAgHEIKAAAwDh+uUkWaMm45wQAfI8RFAAAYBwCCgAAMA4BBQAAGIeAAgAAjMNNskATcXMsWqpj/+2ycBtaAkZQAACAcQgoAADAOAQUAABgHO5BAYA2hntS0BIwggIAAIxDQAEAAMYhoAAAAONwDwpwEqx/AgBnHiMoAADAOAQUAABgHAIKAAAwDgEFAAAYh4ACAACMQ0ABAADGIaAAAADjEFAAAIBxWKgNANo4Hh4IEzGCAgAAjENAAQAAxiGgAAAA43APCnAcPCQQAPyHERQAAGAcRlDQ5jFSAgDm8foISk5OjgYOHKiOHTuqa9euGjNmjIqLiz2OOXjwoDIyMhQZGakOHTpo7NixKi8v93YpAACghfJ6QFm3bp0yMjK0fv16rV69WnV1dbruuutUU1PjPmbq1KlasWKF3nvvPa1bt0779u3TTTfd5O1SAABAC2WzLN8uybN//3517dpV69at05VXXimn06kuXbpoyZIlGjdunCRp27Zt6t27twoKCnTZZZedtE+XyyWHwyGn06mwsDBflo82gJ94AE8s1AZfac73t89vknU6nZKkiIgISVJRUZHq6uqUnJzsPqZXr16Ki4tTQUFBo33U1tbK5XJ5bAAAoPXyaUCpr6/XlClTdMUVV+jCCy+UJJWVlSk4OFjh4eEex0ZFRamsrKzRfnJycuRwONxbbGysL8tGK2ezeW4AAPP4NKBkZGToq6++0tKlS0+rn+zsbDmdTvdWWlrqpQoBAICJfDbNODMzUytXrlR+fr66d+/ubo+OjtahQ4dUVVXlMYpSXl6u6OjoRvuy2+2y2+2+KhUAABjG6yMolmUpMzNTubm5WrNmjRISEjz29+/fX0FBQcrLy3O3FRcXa/fu3UpKSvJ2OQAAoAXy+ghKRkaGlixZouXLl6tjx47u+0ocDodCQ0PlcDiUnp6urKwsRUREKCwsTJMnT1ZSUlKTZvAAAIDWz+vTjG3Huetw0aJFuuuuuyT9vFDbtGnT9M4776i2tlYpKSlasGDBcX/iORbTjHE6uDEWODGmGcNXmvP97fN1UHyBgILTQUABTqzlfSugpTBqHRQAAIDmIqAAAADjEFAAAIBxCCgAAMA4PluoDQDQMp3qjeTcXAtvYgQFAAAYhxEUtHpMKwaAlocRFAAAYBwCCgAAMA4BBQAAGIeAAgAAjENAAQAAxmEWD1oNZusAQOvBCAoAADAOAQUAABiHn3gAAF5xsp9ZWQofzcEICgAAMA4jKACAM+LYERZGVHAijKAAAADjEFAAAIBxCCgAAMA43IOCFouF2QCg9WIEBQAAGIeAAgAAjENAAQAAxuEeFACAX5zufWSso9K6MYICAACMQ0ABAADG4ScenHFMDwbgDTycsHVjBAUAABiHgAIAAIxDQAEAAMbxa0CZP3++evTooZCQECUmJurzzz/3ZzloJpvt1DYAAE7GbwHl3XffVVZWlmbNmqVNmzapb9++SklJUUVFhb9KAgAAhrBZln/uc05MTNTAgQP1+9//XpJUX1+v2NhYTZ48WdOnTz/huS6XSw6HQ06nU2FhYWei3FaNUQ0AaHla4iyl5nx/+2Wa8aFDh1RUVKTs7Gx3W0BAgJKTk1VQUNDg+NraWtXW1rpfO51OST9/UAAA2qKW+BV49Hu7KWMjfgko//rXv3TkyBFFRUV5tEdFRWnbtm0Njs/JydHs2bMbtMfGxvqsRgAATOZw+LuCU3fgwAE5TvIBWsRCbdnZ2crKynK/rq+vV2VlpSIjI2Vrob9PuFwuxcbGqrS0tM3/TMW1+AXX4mdch19wLX7GdfhFS74WlmXpwIEDiomJOemxfgkonTt3Vrt27VReXu7RXl5erujo6AbH2+122e12j7bw8HBflnjGhIWFtbh/YL7CtfgF1+JnXIdfcC1+xnX4RUu9FicbOTnKL7N4goOD1b9/f+Xl5bnb6uvrlZeXp6SkJH+UBAAADOK3n3iysrKUlpamAQMGaNCgQZozZ45qamp09913+6skAABgCL8FlFtvvVX79+/XzJkzVVZWpn79+mnVqlUNbpxtrex2u2bNmtXgp6u2iGvxC67Fz7gOv+Ba/Izr8Iu2ci38tg4KAADA8fAsHgAAYBwCCgAAMA4BBQAAGIeAAgAAjENAAQAAxiGgGGL06NGKi4tTSEiIunXrpgkTJmjfvn3+LuuM2rlzp9LT05WQkKDQ0FD17NlTs2bN0qFDh/xdml88+eSTuvzyy3XWWWe1mpWTm2r+/Pnq0aOHQkJClJiYqM8//9zfJZ1x+fn5GjVqlGJiYmSz2bRs2TJ/l+QXOTk5GjhwoDp27KiuXbtqzJgxKi4u9ndZfvHyyy/r4osvdq8gm5SUpL/97W/+LstnCCiGGDp0qP74xz+quLhYH3zwgXbs2KFx48b5u6wzatu2baqvr9crr7yirVu36qWXXtLChQv129/+1t+l+cWhQ4d0880364EHHvB3KWfUu+++q6ysLM2aNUubNm1S3759lZKSooqKCn+XdkbV1NSob9++mj9/vr9L8at169YpIyND69ev1+rVq1VXV6frrrtONTU1/i7tjOvevbuefvppFRUVaePGjbr66qt1ww03aOvWrf4uzTcsGGn58uWWzWazDh065O9S/OrZZ5+1EhIS/F2GXy1atMhyOBz+LuOMGTRokJWRkeF+feTIESsmJsbKycnxY1X+JcnKzc31dxlGqKiosCRZ69at83cpRujUqZP13//93/4uwycYQTFQZWWl3n77bV1++eUKCgrydzl+5XQ6FRER4e8ycIYcOnRIRUVFSk5OdrcFBAQoOTlZBQUFfqwMpnA6nZLU5v8uHDlyREuXLlVNTU2rfYYdAcUgjz76qNq3b6/IyEjt3r1by5cv93dJfrV9+3bNmzdP9913n79LwRnyr3/9S0eOHGnwyIuoqCiVlZX5qSqYor6+XlOmTNEVV1yhCy+80N/l+MWXX36pDh06yG636/7771dubq769Onj77J8goDiQ9OnT5fNZjvhtm3bNvfxDz/8sP7xj3/oo48+Urt27XTnnXfKagVPImjudZCkvXv3atiwYbr55pt17733+qly7zuVawHgZxkZGfrqq6+0dOlSf5fiN+eff742b96swsJCPfDAA0pLS9PXX3/t77J8gmfx+ND+/fv1/fffn/CYs88+W8HBwQ3a9+zZo9jYWH322Wctfviuuddh3759uuqqq3TZZZdp8eLFCghoPTn6VP5NLF68WFOmTFFVVZWPq/O/Q4cO6ayzztL777+vMWPGuNvT0tJUVVXVZkcVbTabcnNzPa5JW5OZmanly5crPz9fCQkJ/i7HGMnJyerZs6deeeUVf5fidX57mnFb0KVLF3Xp0uWUzq2vr5ck1dbWerMkv2jOddi7d6+GDh2q/v37a9GiRa0qnEin92+iLQgODlb//v2Vl5fn/jKur69XXl6eMjMz/Vsc/MKyLE2ePFm5ublau3Yt4eQY9fX1reJ7ojEEFAMUFhZqw4YNGjx4sDp16qQdO3ZoxowZ6tmzZ4sfPWmOvXv36qqrrlJ8fLyef/557d+/370vOjraj5X5x+7du1VZWandu3fryJEj2rx5syTpnHPOUYcOHfxbnA9lZWUpLS1NAwYM0KBBgzRnzhzV1NTo7rvv9ndpZ1R1dbW2b9/ufl1SUqLNmzcrIiJCcXFxfqzszMrIyNCSJUu0fPlydezY0X0vksPhUGhoqJ+rO7Oys7M1fPhwxcXF6cCBA1qyZInWrl2rDz/80N+l+YZ/JxHBsixry5Yt1tChQ62IiAjLbrdbPXr0sO6//35rz549/i7tjFq0aJElqdGtLUpLS2v0Wnz88cf+Ls3n5s2bZ8XFxVnBwcHWoEGDrPXr1/u7pDPu448/bvS/f1pamr9LO6OO9zdh0aJF/i7tjJs4caIVHx9vBQcHW126dLGuueYa66OPPvJ3WT7DPSgAAMA4resHfgAA0CoQUAAAgHEIKAAAwDgEFAAAYBwCCgAAMA4BBQAAGIeAAgAAjENAAQAAxiGgAAAA4xBQAACAcQgoAADAOP8PcjScXT7ELgEAAAAASUVORK5CYII="
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now we are taking `num_head = 8`, hence we'll reshape the QKV matrix accordingly. The head dimensions will be the d_model by the num_heads, hence we can shape it according to number of heads (to divide the data in the multiple heads) and 3 * the head dimensions to get Q, K and V",
   "id": "127d9acabfbd43fd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T13:07:47.455035Z",
     "start_time": "2025-07-25T13:07:47.433570Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_heads = 8\n",
    "head_dim = d_model // num_heads\n",
    "QKV = QKV.reshape(batch_size, seq_len, num_heads, 3*head_dim)"
   ],
   "id": "5c0f6cab672e8ac3",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T13:08:01.579624Z",
     "start_time": "2025-07-25T13:08:01.568590Z"
    }
   },
   "cell_type": "code",
   "source": "QKV.shape",
   "id": "9f02777b47a31744",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 8, 192])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Changing the dimensions from `seq_len, num_heads` to `num_heads, seq_len` for the ease of processing on the last two dimensions",
   "id": "a5c430d4446855ad"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T13:20:31.211851Z",
     "start_time": "2025-07-25T13:20:31.152874Z"
    }
   },
   "cell_type": "code",
   "source": [
    "QKV = QKV.permute(0,2,1,3)\n",
    "QKV.shape"
   ],
   "id": "fb80f147c8afe197",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 4, 192])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T13:22:09.466666Z",
     "start_time": "2025-07-25T13:22:09.456401Z"
    }
   },
   "cell_type": "code",
   "source": [
    "Q, K, V = QKV.chunk(3, dim=-1) # Breaking by last dimension hence dim=-1\n",
    "Q.shape, K.shape, V.shape"
   ],
   "id": "af1eb535095dbc16",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 8, 4, 64]),\n",
       " torch.Size([1, 8, 4, 64]),\n",
       " torch.Size([1, 8, 4, 64]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Implementing Self Attention for Multiple Heads\n",
    "\n",
    "For a single head:\n",
    "\n",
    "$$\n",
    "\\text{self attention} = softmax\\bigg(\\frac{Q.K^T}{\\sqrt{d_k}}+M\\bigg)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{new V} = \\text{self attention}.V\n",
    "$$"
   ],
   "id": "9426aa44bb2039ec"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T14:23:57.190578Z",
     "start_time": "2025-07-25T14:23:57.092264Z"
    }
   },
   "cell_type": "code",
   "source": [
    "d_k = Q.size()[-1]\n",
    "stabilized = torch.matmul(Q, K.transpose(-2, -1))/ np.sqrt(d_k)\n",
    "stabilized.shape"
   ],
   "id": "96a8e6fc40f2b125",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 4, 4])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### MAsking\n",
    "\n",
    "We'll mask the data so as the decoder doesn't cheat in decoding."
   ],
   "id": "4d22e9701328c175"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T14:29:25.685297Z",
     "start_time": "2025-07-25T14:29:25.657744Z"
    }
   },
   "cell_type": "code",
   "source": [
    "mask = torch.full(stabilized.shape, float('-inf'))\n",
    "mask = torch.triu(mask, diagonal=1)  # Used for creating triangular matrix"
   ],
   "id": "b3ca0f36d869e5cd",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T14:30:20.240065Z",
     "start_time": "2025-07-25T14:30:20.230825Z"
    }
   },
   "cell_type": "code",
   "source": "mask[0][0] # Mask for a input for single head",
   "id": "d8b464abcbd3602a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf],\n",
       "        [0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T14:30:51.178118Z",
     "start_time": "2025-07-25T14:30:51.165905Z"
    }
   },
   "cell_type": "code",
   "source": "(stabilized+mask)[0][0]",
   "id": "6a32603c5eb3a7a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3259,    -inf,    -inf,    -inf],\n",
       "        [ 0.4438,  0.2990,    -inf,    -inf],\n",
       "        [-0.0922,  0.6536,  0.3198,    -inf],\n",
       "        [ 0.1699,  0.1053, -0.3614, -0.2198]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T14:32:10.617082Z",
     "start_time": "2025-07-25T14:32:10.604935Z"
    }
   },
   "cell_type": "code",
   "source": "stabilized += mask",
   "id": "5057a5fb6506c360",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now we perform the softmax operation to convert the data into probability distribution, (as e^(-inf) = 0), hence the decoder wont be able to take it's context)",
   "id": "a54e0334f705b4db"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We have a builtin Softmax function from Torch's neural networks, and its implemented the same way as we have done above.",
   "id": "f83c02c82b0ef46c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T14:36:08.423160Z",
     "start_time": "2025-07-25T14:36:08.413656Z"
    }
   },
   "cell_type": "code",
   "source": "attention = F.softmax(stabilized, dim=-1)",
   "id": "a9e7ca528a445f93",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T14:36:09.387471Z",
     "start_time": "2025-07-25T14:36:09.373141Z"
    }
   },
   "cell_type": "code",
   "source": "attention.shape",
   "id": "235b5a7ebf5410da",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 4, 4])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T14:36:09.997980Z",
     "start_time": "2025-07-25T14:36:09.988420Z"
    }
   },
   "cell_type": "code",
   "source": "attention[0][0]",
   "id": "ef2b854e6f6a249a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5361, 0.4639, 0.0000, 0.0000],\n",
       "        [0.2165, 0.4565, 0.3270, 0.0000],\n",
       "        [0.3123, 0.2927, 0.1836, 0.2115]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Now we generate new Val vector as above",
   "id": "cf5ba7ad18473606"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T14:37:20.582333Z",
     "start_time": "2025-07-25T14:37:20.552036Z"
    }
   },
   "cell_type": "code",
   "source": [
    "newVal = torch.matmul(attention, V)\n",
    "newVal.shape"
   ],
   "id": "7fca8b34a7b61b2a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 4, 64])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "So if the Q, K, V matrices are given, then the function we have here is as given below:",
   "id": "f5ffe9108b2baadb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T14:40:14.437391Z",
     "start_time": "2025-07-25T14:40:14.410443Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def multihead_attention(Q, K, V, mask = None):\n",
    "    d_k = Q.shape[-1]\n",
    "    stabilized = torch.matmul(Q, K.transpose(-2, -1))/ np.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        stabilized = stabilized+mask\n",
    "    attention = F.softmax(stabilized, dim=-1)\n",
    "    Output = torch.matmul(attention, V)\n",
    "    return Output"
   ],
   "id": "c11588670c01059b",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T14:41:01.527420Z",
     "start_time": "2025-07-25T14:41:01.510126Z"
    }
   },
   "cell_type": "code",
   "source": [
    "out = multihead_attention(Q, K, V, mask= None)\n",
    "out"
   ],
   "id": "14be36881e4444ce",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.4978e-02,  2.3663e-01,  1.6114e-01,  ..., -6.9181e-01,\n",
       "            5.7441e-01,  6.5830e-02],\n",
       "          [-2.1464e-02,  2.8223e-01,  1.7461e-01,  ..., -6.5455e-01,\n",
       "            6.2107e-01,  3.3795e-02],\n",
       "          [-1.8537e-01,  2.5994e-01,  2.3923e-01,  ..., -7.2543e-01,\n",
       "            7.4261e-01,  1.8783e-01],\n",
       "          [ 5.2140e-04,  2.6070e-01,  1.7505e-01,  ..., -6.3518e-01,\n",
       "            6.0713e-01,  4.7038e-02]],\n",
       "\n",
       "         [[ 1.8249e-01, -4.5593e-01,  4.2315e-02,  ..., -1.7035e-01,\n",
       "           -3.6548e-01, -4.2218e-01],\n",
       "          [ 1.3278e-01, -3.2880e-01, -1.1906e-02,  ..., -1.9151e-01,\n",
       "           -4.2904e-01, -4.1078e-01],\n",
       "          [ 2.8622e-01, -5.4838e-01, -7.7616e-03,  ..., -3.2329e-01,\n",
       "           -2.8535e-01, -5.9498e-01],\n",
       "          [ 1.8461e-01, -7.2107e-01,  1.6522e-01,  ..., -1.4510e-01,\n",
       "           -3.8200e-01, -1.2629e-01]],\n",
       "\n",
       "         [[-1.0624e-01, -1.0433e-01,  3.6652e-02,  ..., -9.5584e-02,\n",
       "            3.9210e-01,  4.3972e-01],\n",
       "          [-2.6631e-01, -8.8138e-02, -1.4662e-02,  ..., -1.4589e-01,\n",
       "            4.4522e-01,  4.5144e-01],\n",
       "          [-1.6021e-01, -5.4223e-02, -4.1146e-02,  ..., -1.0521e-01,\n",
       "            3.2036e-01,  4.7894e-01],\n",
       "          [-3.4725e-01,  3.7255e-03, -7.5556e-03,  ..., -7.3805e-03,\n",
       "            2.7087e-01,  4.8859e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-2.0335e-01,  5.0393e-01, -3.5698e-01,  ..., -1.4027e-02,\n",
       "           -2.4244e-01,  5.6141e-03],\n",
       "          [-5.7250e-02,  7.2077e-01, -3.0302e-01,  ...,  4.1060e-02,\n",
       "           -3.5983e-01,  6.2665e-03],\n",
       "          [-2.8136e-01,  4.6643e-01, -2.2611e-01,  ...,  2.2505e-02,\n",
       "           -2.1522e-01,  3.3053e-02],\n",
       "          [-3.9552e-01,  3.9178e-01, -3.7385e-01,  ...,  1.4451e-02,\n",
       "           -1.7516e-01,  9.5126e-04]],\n",
       "\n",
       "         [[-1.7901e-01, -6.7010e-03,  2.4194e-01,  ..., -8.1206e-02,\n",
       "            2.2674e-02,  1.8792e-01],\n",
       "          [-1.8444e-01, -4.9970e-02,  2.3730e-01,  ..., -1.0364e-01,\n",
       "            7.1614e-02,  1.9373e-01],\n",
       "          [-6.2457e-02,  3.5987e-02,  4.1040e-01,  ...,  8.1470e-02,\n",
       "           -1.3965e-01,  2.3324e-01],\n",
       "          [-1.3319e-01, -1.0115e-01,  3.1488e-01,  ..., -6.6613e-02,\n",
       "            6.5877e-02,  2.2612e-01]],\n",
       "\n",
       "         [[ 1.0777e-01,  3.1227e-02, -1.1589e-01,  ..., -4.9841e-01,\n",
       "           -5.1086e-02, -2.3020e-01],\n",
       "          [ 2.0875e-01,  7.6120e-02, -1.0718e-01,  ..., -6.1810e-01,\n",
       "           -1.3791e-02, -2.8526e-01],\n",
       "          [-8.9988e-06, -2.5488e-02, -8.4474e-02,  ..., -3.6620e-01,\n",
       "           -4.5840e-02, -1.8443e-01],\n",
       "          [ 1.4566e-01,  6.2443e-02, -8.5508e-02,  ..., -5.1537e-01,\n",
       "            9.1060e-04, -2.5255e-01]]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T14:41:18.312985Z",
     "start_time": "2025-07-25T14:41:18.301610Z"
    }
   },
   "cell_type": "code",
   "source": [
    "out = multihead_attention(Q, K, V, mask)\n",
    "out"
   ],
   "id": "e79670c69f0a12ef",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 6.4529e-01,  3.1930e-01, -1.3173e-01,  ..., -6.0471e-01,\n",
       "            4.4914e-02, -5.3154e-01],\n",
       "          [ 1.7461e-01,  1.0009e-01,  2.4083e-02,  ..., -1.1043e+00,\n",
       "            2.8910e-01,  1.0770e-01],\n",
       "          [-2.6310e-01,  2.4970e-01,  2.2142e-01,  ..., -9.5741e-01,\n",
       "            7.3394e-01,  2.5061e-01],\n",
       "          [ 5.2140e-04,  2.6070e-01,  1.7505e-01,  ..., -6.3518e-01,\n",
       "            6.0713e-01,  4.7038e-02]],\n",
       "\n",
       "         [[ 7.3822e-01, -1.0076e+00, -9.0070e-02,  ..., -7.4522e-01,\n",
       "            1.4601e-01, -1.3474e+00],\n",
       "          [ 3.9463e-01, -7.0996e-01,  1.0327e-01,  ..., -1.8662e-01,\n",
       "           -1.0153e-01, -7.7890e-01],\n",
       "          [ 3.2883e-01, -3.5622e-01, -1.2717e-01,  ..., -3.9021e-01,\n",
       "           -2.2907e-01, -9.3328e-01],\n",
       "          [ 1.8461e-01, -7.2107e-01,  1.6522e-01,  ..., -1.4510e-01,\n",
       "           -3.8200e-01, -1.2629e-01]],\n",
       "\n",
       "         [[-7.8245e-01,  1.3699e-01,  1.3189e-01,  ...,  2.8067e-01,\n",
       "            1.4551e-01,  4.9573e-01],\n",
       "          [-1.0694e-01,  1.2857e-02,  1.9435e-01,  ...,  2.4817e-01,\n",
       "            8.5433e-02,  4.5691e-01],\n",
       "          [-1.3994e-01,  1.1340e-01, -1.4194e-01,  ...,  5.4792e-02,\n",
       "           -5.4874e-02,  5.8228e-01],\n",
       "          [-3.4725e-01,  3.7255e-03, -7.5556e-03,  ..., -7.3805e-03,\n",
       "            2.7087e-01,  4.8859e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-2.4939e-01,  1.0954e-01, -9.6641e-01,  ..., -2.9515e-01,\n",
       "           -5.5602e-02, -9.4465e-02],\n",
       "          [-7.2711e-01, -2.0360e-02, -3.5543e-01,  ..., -4.6527e-02,\n",
       "            5.4378e-02,  2.3001e-02],\n",
       "          [-3.4178e-01,  2.8751e-01, -1.8171e-01,  ..., -3.8137e-02,\n",
       "           -1.1899e-01,  5.5392e-02],\n",
       "          [-3.9552e-01,  3.9178e-01, -3.7385e-01,  ...,  1.4451e-02,\n",
       "           -1.7516e-01,  9.5126e-04]],\n",
       "\n",
       "         [[ 3.0860e-01,  9.0319e-02,  9.8360e-01,  ...,  7.1745e-01,\n",
       "           -5.0423e-01,  4.0814e-01],\n",
       "          [ 2.4843e-02, -2.0427e-01,  5.6802e-01,  ...,  1.5493e-01,\n",
       "            3.5317e-02,  3.2443e-01],\n",
       "          [-1.0078e-02,  6.7924e-03,  5.1696e-01,  ...,  2.6787e-01,\n",
       "           -1.0167e-01,  2.7618e-01],\n",
       "          [-1.3319e-01, -1.0115e-01,  3.1488e-01,  ..., -6.6613e-02,\n",
       "            6.5877e-02,  2.2612e-01]],\n",
       "\n",
       "         [[ 7.4496e-01,  3.3388e-01, -1.6504e-01,  ..., -1.2690e+00,\n",
       "            6.7808e-02, -5.4551e-01],\n",
       "          [ 4.1320e-01,  2.0334e-01, -1.3516e-01,  ..., -8.3277e-01,\n",
       "            2.1698e-02, -3.7306e-01],\n",
       "          [-6.2265e-02, -1.7544e-02,  7.4715e-02,  ..., -1.8651e-01,\n",
       "            1.4143e-01, -1.7851e-01],\n",
       "          [ 1.4566e-01,  6.2443e-02, -8.5508e-02,  ..., -5.1537e-01,\n",
       "            9.1060e-04, -2.5255e-01]]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Combining the results of all the heads and making it same as input dimension",
   "id": "8aaf3ea93f7ec9fd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T14:44:24.132730Z",
     "start_time": "2025-07-25T14:44:24.086940Z"
    }
   },
   "cell_type": "code",
   "source": [
    "out = out.reshape(batch_size, seq_len, num_heads*head_dim)\n",
    "out.shape"
   ],
   "id": "154afef91b12151a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 512])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "to share the results of the patterns by these heads, we'll use a linear model so the heads can communicate.",
   "id": "94b7decd3895d9bf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T14:45:48.293960Z",
     "start_time": "2025-07-25T14:45:48.242677Z"
    }
   },
   "cell_type": "code",
   "source": "LL = nn.Linear(d_model, d_model)",
   "id": "3179591ba37c01a0",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T14:45:55.747637Z",
     "start_time": "2025-07-25T14:45:55.710016Z"
    }
   },
   "cell_type": "code",
   "source": "out = LL(out)",
   "id": "f5f99d7a4b5fce25",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T14:45:58.931379Z",
     "start_time": "2025-07-25T14:45:58.924188Z"
    }
   },
   "cell_type": "code",
   "source": "out.shape",
   "id": "947d809bf4420cbb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 512])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now the output we have is very much context aware, we can improve this whole structure by implementing Multihead Attention as a class.",
   "id": "9955127537677a66"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T15:00:15.858559Z",
     "start_time": "2025-07-25T15:00:15.800371Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int, input_dim: int):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.input_dim = input_dim\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.QKV_layer = nn.Linear(input_dim, 3*d_model)\n",
    "        self.LinearLayer = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, data, mask=None):\n",
    "        batch_size, seq_len, input_dim = data.size()\n",
    "        print(f\"data.size()= {data.size()}\")\n",
    "        QKV = self.QKV_layer(data)\n",
    "        print(f\"QKV.siz()= {QKV.size()}\")\n",
    "        QKV = QKV.reshape(batch_size, seq_len, self.num_heads, 3*self.head_dim)\n",
    "        print(f\"QKV.siz()= {QKV.size()}\")\n",
    "        QKV = QKV.permute(0,2,1,3)\n",
    "        print(f\"QKV.siz()= {QKV.size()}\")\n",
    "        Q, K, V = QKV.chunk(3, dim=-1)\n",
    "        print(f\"Q.size()= {Q.size()}, K.size()= {K.size()}, V.size()= {V.size()}\")\n",
    "        newVal = multihead_attention(Q, K, V, mask)\n",
    "        print(f\"newVal.size()= {newVal.size()}\")\n",
    "        newVal = newVal.reshape(batch_size, seq_len, self.num_heads*self.head_dim)\n",
    "        print(f\"newVal.size()= {newVal.size()}\")\n",
    "        Output = self.LinearLayer(newVal)\n",
    "        print(f\"Output.size()= {Output.size()}\")\n",
    "        return Output"
   ],
   "id": "6ec31c47c36c550e",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Sample input for the crosscheck of its functioning",
   "id": "524beceb25f4ce6f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T15:03:19.186570Z",
     "start_time": "2025-07-25T15:03:18.902789Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_dim = 1024\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "\n",
    "batch_size = 20\n",
    "seq_len = 6\n",
    "X = torch.randn((batch_size, seq_len, input_dim))\n",
    "\n",
    "model = MultiHeadAttention(d_model, num_heads, input_dim)\n",
    "output = model.forward(X)"
   ],
   "id": "74a537f759993dcc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data.size()= torch.Size([20, 6, 1024])\n",
      "QKV.siz()= torch.Size([20, 6, 1536])\n",
      "QKV.siz()= torch.Size([20, 6, 8, 192])\n",
      "QKV.siz()= torch.Size([20, 8, 6, 192])\n",
      "Q.size()= torch.Size([20, 8, 6, 64]), K.size()= torch.Size([20, 8, 6, 64]), V.size()= torch.Size([20, 8, 6, 64])\n",
      "newVal.size()= torch.Size([20, 8, 6, 64])\n",
      "newVal.size()= torch.Size([20, 6, 512])\n",
      "Output.size()= torch.Size([20, 6, 512])\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Implementation of Positional Encoding phase",
   "id": "22de8aba84fea14d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-26T18:02:28.700190Z",
     "start_time": "2025-07-26T18:02:25.595241Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "max_sequence = 10\n",
    "d_model = 6"
   ],
   "id": "26f9e9bb82cf8ddf",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-26T18:08:12.966995Z",
     "start_time": "2025-07-26T18:08:12.867872Z"
    }
   },
   "cell_type": "code",
   "source": "even_i = torch.arange(0, d_model, 2).float()",
   "id": "90e58b10dad99912",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-26T18:08:21.104604Z",
     "start_time": "2025-07-26T18:08:20.954685Z"
    }
   },
   "cell_type": "code",
   "source": "even_i",
   "id": "597952a98df6ac92",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 2., 4.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-26T18:10:03.830072Z",
     "start_time": "2025-07-26T18:10:03.817106Z"
    }
   },
   "cell_type": "code",
   "source": [
    "even_denominator = torch.pow(10000, even_i/d_model)\n",
    "even_denominator"
   ],
   "id": "8cf272eaaac1bcdf",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  1.0000,  21.5443, 464.1590])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-26T18:10:28.394527Z",
     "start_time": "2025-07-26T18:10:28.385540Z"
    }
   },
   "cell_type": "code",
   "source": [
    "odd_i = torch.arange(1, d_model, 2).float()\n",
    "odd_i"
   ],
   "id": "b67521164b782da8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 3., 5.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-26T18:10:34.622360Z",
     "start_time": "2025-07-26T18:10:34.612253Z"
    }
   },
   "cell_type": "code",
   "source": [
    "odd_denominator = torch.pow(10000, (odd_i-1)/d_model)\n",
    "odd_denominator"
   ],
   "id": "41afaaae422a569b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  1.0000,  21.5443, 464.1590])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Since the denominators are same for both of them, we'll use any one as our denominator",
   "id": "66d1b7bda26bb319"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-26T18:12:19.447792Z",
     "start_time": "2025-07-26T18:12:19.431982Z"
    }
   },
   "cell_type": "code",
   "source": "denominator = even_denominator",
   "id": "4f565b8b178bcd65",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-26T18:13:44.095222Z",
     "start_time": "2025-07-26T18:13:44.064534Z"
    }
   },
   "cell_type": "code",
   "source": "position = torch.arange(max_sequence, dtype=torch.float).reshape(max_sequence, 1)",
   "id": "b6839c16a66ffe13",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-26T18:13:48.444110Z",
     "start_time": "2025-07-26T18:13:48.412956Z"
    }
   },
   "cell_type": "code",
   "source": "position",
   "id": "d6d4372d0686537c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [1.],\n",
       "        [2.],\n",
       "        [3.],\n",
       "        [4.],\n",
       "        [5.],\n",
       "        [6.],\n",
       "        [7.],\n",
       "        [8.],\n",
       "        [9.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-26T18:14:35.661620Z",
     "start_time": "2025-07-26T18:14:35.629513Z"
    }
   },
   "cell_type": "code",
   "source": [
    "even_pe = torch.sin(position/denominator)\n",
    "odd_pe = torch.cos(position/denominator)"
   ],
   "id": "f94cc1a693761536",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-26T18:14:39.105106Z",
     "start_time": "2025-07-26T18:14:39.087111Z"
    }
   },
   "cell_type": "code",
   "source": "even_pe",
   "id": "6a4e6d6c0b9f7fad",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.8415,  0.0464,  0.0022],\n",
       "        [ 0.9093,  0.0927,  0.0043],\n",
       "        [ 0.1411,  0.1388,  0.0065],\n",
       "        [-0.7568,  0.1846,  0.0086],\n",
       "        [-0.9589,  0.2300,  0.0108],\n",
       "        [-0.2794,  0.2749,  0.0129],\n",
       "        [ 0.6570,  0.3192,  0.0151],\n",
       "        [ 0.9894,  0.3629,  0.0172],\n",
       "        [ 0.4121,  0.4057,  0.0194]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-26T18:16:03.349621Z",
     "start_time": "2025-07-26T18:16:03.317621Z"
    }
   },
   "cell_type": "code",
   "source": [
    "stacked = torch.stack([even_pe, odd_pe], dim=2)\n",
    "stacked.shape, stacked"
   ],
   "id": "c97830dd52be777",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 3, 2]),\n",
       " tensor([[[ 0.0000,  1.0000],\n",
       "          [ 0.0000,  1.0000],\n",
       "          [ 0.0000,  1.0000]],\n",
       " \n",
       "         [[ 0.8415,  0.5403],\n",
       "          [ 0.0464,  0.9989],\n",
       "          [ 0.0022,  1.0000]],\n",
       " \n",
       "         [[ 0.9093, -0.4161],\n",
       "          [ 0.0927,  0.9957],\n",
       "          [ 0.0043,  1.0000]],\n",
       " \n",
       "         [[ 0.1411, -0.9900],\n",
       "          [ 0.1388,  0.9903],\n",
       "          [ 0.0065,  1.0000]],\n",
       " \n",
       "         [[-0.7568, -0.6536],\n",
       "          [ 0.1846,  0.9828],\n",
       "          [ 0.0086,  1.0000]],\n",
       " \n",
       "         [[-0.9589,  0.2837],\n",
       "          [ 0.2300,  0.9732],\n",
       "          [ 0.0108,  0.9999]],\n",
       " \n",
       "         [[-0.2794,  0.9602],\n",
       "          [ 0.2749,  0.9615],\n",
       "          [ 0.0129,  0.9999]],\n",
       " \n",
       "         [[ 0.6570,  0.7539],\n",
       "          [ 0.3192,  0.9477],\n",
       "          [ 0.0151,  0.9999]],\n",
       " \n",
       "         [[ 0.9894, -0.1455],\n",
       "          [ 0.3629,  0.9318],\n",
       "          [ 0.0172,  0.9999]],\n",
       " \n",
       "         [[ 0.4121, -0.9111],\n",
       "          [ 0.4057,  0.9140],\n",
       "          [ 0.0194,  0.9998]]]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-26T18:16:42.242624Z",
     "start_time": "2025-07-26T18:16:42.224621Z"
    }
   },
   "cell_type": "code",
   "source": [
    "PE = torch.flatten(stacked, start_dim=1, end_dim=2)\n",
    "PE"
   ],
   "id": "259e769419fa5521",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  1.0000,  0.0000,  1.0000,  0.0000,  1.0000],\n",
       "        [ 0.8415,  0.5403,  0.0464,  0.9989,  0.0022,  1.0000],\n",
       "        [ 0.9093, -0.4161,  0.0927,  0.9957,  0.0043,  1.0000],\n",
       "        [ 0.1411, -0.9900,  0.1388,  0.9903,  0.0065,  1.0000],\n",
       "        [-0.7568, -0.6536,  0.1846,  0.9828,  0.0086,  1.0000],\n",
       "        [-0.9589,  0.2837,  0.2300,  0.9732,  0.0108,  0.9999],\n",
       "        [-0.2794,  0.9602,  0.2749,  0.9615,  0.0129,  0.9999],\n",
       "        [ 0.6570,  0.7539,  0.3192,  0.9477,  0.0151,  0.9999],\n",
       "        [ 0.9894, -0.1455,  0.3629,  0.9318,  0.0172,  0.9999],\n",
       "        [ 0.4121, -0.9111,  0.4057,  0.9140,  0.0194,  0.9998]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Positional Encoding Implementation as Class",
   "id": "2982bb488f7540ac"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-26T18:22:03.808281Z",
     "start_time": "2025-07-26T18:22:03.754109Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_sequence_length):\n",
    "        super().__init__()\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self):\n",
    "        even_i = torch.arange(0, self.d_model, 2).float()\n",
    "        denominator = torch.pow(10000, even_i/self.d_model)\n",
    "        position = torch.arange(self.max_sequence_length).reshape(self.max_sequence_length, 1)\n",
    "        even_pe = torch.sin(position/denominator)\n",
    "        odd_pe = torch.cos(position/denominator)\n",
    "        stacked = torch.stack([even_pe, odd_pe], dim=2)\n",
    "        PE = torch.flatten(stacked, start_dim=1, end_dim=2)\n",
    "        return PE"
   ],
   "id": "fc77ce3e9992596",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-26T18:22:35.973876Z",
     "start_time": "2025-07-26T18:22:35.903076Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pe = PositionalEncoding(d_model = 6, max_sequence_length = 10)\n",
    "PE = pe.forward()\n",
    "PE.shape"
   ],
   "id": "1d77969c142e089c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 6])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-26T18:22:41.977806Z",
     "start_time": "2025-07-26T18:22:41.940411Z"
    }
   },
   "cell_type": "code",
   "source": "PE",
   "id": "1e5df9950f7c267b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  1.0000,  0.0000,  1.0000,  0.0000,  1.0000],\n",
       "        [ 0.8415,  0.5403,  0.0464,  0.9989,  0.0022,  1.0000],\n",
       "        [ 0.9093, -0.4161,  0.0927,  0.9957,  0.0043,  1.0000],\n",
       "        [ 0.1411, -0.9900,  0.1388,  0.9903,  0.0065,  1.0000],\n",
       "        [-0.7568, -0.6536,  0.1846,  0.9828,  0.0086,  1.0000],\n",
       "        [-0.9589,  0.2837,  0.2300,  0.9732,  0.0108,  0.9999],\n",
       "        [-0.2794,  0.9602,  0.2749,  0.9615,  0.0129,  0.9999],\n",
       "        [ 0.6570,  0.7539,  0.3192,  0.9477,  0.0151,  0.9999],\n",
       "        [ 0.9894, -0.1455,  0.3629,  0.9318,  0.0172,  0.9999],\n",
       "        [ 0.4121, -0.9111,  0.4057,  0.9140,  0.0194,  0.9998]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Layer Normalization\n",
    "\n",
    "It is a technique to convert the data into normalized form, this step comes after Multihead Attention and it takes the output of Multihead Attention as well as the Positional Encoding vector as input. We make the data such that the mena is close or equal to 0 and standard deviation is close to 1."
   ],
   "id": "c4b772cda23f31d6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T13:50:05.276009Z",
     "start_time": "2025-07-27T13:50:01.139839Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch import nn"
   ],
   "id": "c60b02af11a185ac",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T13:53:08.869733Z",
     "start_time": "2025-07-27T13:53:08.601323Z"
    }
   },
   "cell_type": "code",
   "source": [
    "inputs = torch.Tensor([[[0.2,0.1,0.3], [0.5,0.1,0.1]]])\n",
    "B, S, E = inputs.size()\n",
    "inputs = inputs.reshape(S, B, E)\n",
    "inputs.size(), inputs"
   ],
   "id": "7dd2396c0200ec9e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 1, 3]),\n",
       " tensor([[[0.2000, 0.1000, 0.3000]],\n",
       " \n",
       "         [[0.5000, 0.1000, 0.1000]]]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T13:57:48.187828Z",
     "start_time": "2025-07-27T13:57:48.157098Z"
    }
   },
   "cell_type": "code",
   "source": [
    "parameter_shape =inputs.size()[-2:]\n",
    "gamma = nn.Parameter(torch.ones(parameter_shape))\n",
    "beta = nn.Parameter(torch.zeros(parameter_shape))"
   ],
   "id": "52045a2a0e12cce9",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T13:58:13.163595Z",
     "start_time": "2025-07-27T13:58:13.147273Z"
    }
   },
   "cell_type": "code",
   "source": "gamma.size(), beta.size()",
   "id": "4db07866936880ae",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3]), torch.Size([1, 3]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T13:58:49.088223Z",
     "start_time": "2025-07-27T13:58:49.077985Z"
    }
   },
   "cell_type": "code",
   "source": "dims = [-(i+1) for i in range(len(parameter_shape))]",
   "id": "da5fbab34840f4bb",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T13:59:49.895051Z",
     "start_time": "2025-07-27T13:59:49.869035Z"
    }
   },
   "cell_type": "code",
   "source": [
    "mean = inputs.mean(dim = dims, keepdim = True)\n",
    "mean.size(), mean"
   ],
   "id": "56633a0c362989e8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 1, 1]),\n",
       " tensor([[[0.2000]],\n",
       " \n",
       "         [[0.2333]]]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T14:09:14.625648Z",
     "start_time": "2025-07-27T14:09:14.578367Z"
    }
   },
   "cell_type": "code",
   "source": [
    "variance = ((inputs-mean)**2).mean(dim = dims, keepdim = True)\n",
    "epsilon = 1e-5\n",
    "std = (variance+epsilon).sqrt()\n",
    "std"
   ],
   "id": "60cd594c8cdf582c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0817]],\n",
       "\n",
       "        [[0.1886]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T14:10:26.459862Z",
     "start_time": "2025-07-27T14:10:26.440522Z"
    }
   },
   "cell_type": "code",
   "source": [
    " y = (inputs-mean)/std\n",
    " y"
   ],
   "id": "fc60bad4c99e2607",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000, -1.2238,  1.2238]],\n",
       "\n",
       "        [[ 1.4140, -0.7070, -0.7070]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now the data is normalized",
   "id": "998df93eb143c44d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T14:11:09.658727Z",
     "start_time": "2025-07-27T14:11:09.631934Z"
    }
   },
   "cell_type": "code",
   "source": [
    "out = gamma*y + beta\n",
    "out"
   ],
   "id": "f7ca754f5b4f413a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000, -1.2238,  1.2238]],\n",
       "\n",
       "        [[ 1.4140, -0.7070, -0.7070]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This additional variable we have here `grad_fn=<AddBackward0>` is a learnable parameter which updates Beta nd Gamma during back propagation phase",
   "id": "e5b7667f8949f693"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Implementing it as as class",
   "id": "de2766a04511e427"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T14:23:38.081271Z",
     "start_time": "2025-07-27T14:23:37.996412Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LayerNormalization():\n",
    "    def __init__(self, parameters_shape, epsilon=1e-5):\n",
    "        self.parameters_shape = parameters_shape\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = nn.Parameter(torch.ones(parameters_shape))\n",
    "        self.beta = nn.Parameter(torch.zeros(parameters_shape))\n",
    "\n",
    "    def forward(self, input):\n",
    "        dims = [-(i+1) for i in range(len(self.parameters_shape))]\n",
    "        mean = input.mean(dims, keepdim = True)\n",
    "        print(f\"mean.size()= {mean.size()}\\n mean= {mean}\")\n",
    "        var = ((inputs-mean)**2).mean(dim=dims, keepdim = True)\n",
    "        std = (var+self.epsilon).sqrt()\n",
    "        print(f\"Standard Deviation, size= {std.size()}\\n std= {std}\")\n",
    "        y = (input-mean)/std\n",
    "        print(f\"y.size()= {y.size()}\\n y= {y}\")\n",
    "        out = self.gamma*y + self.beta\n",
    "        print(f\"out.size()= {out.size()}\\n out= {out}\")\n",
    "        return out"
   ],
   "id": "d4fe27f4671840bd",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Testing the implementation",
   "id": "f8acc00d668f18ab"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T14:26:13.698546Z",
     "start_time": "2025-07-27T14:26:13.618268Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size = 3\n",
    "sentence_length = 5\n",
    "embedding_dim = 8\n",
    "inputs = torch.randn(sentence_length, batch_size, embedding_dim)\n",
    "print(inputs.size)\n",
    "print(inputs)"
   ],
   "id": "fd4f5a121e5c72d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in method size of Tensor object at 0x00000207BD9A4170>\n",
      "tensor([[[-0.3961,  0.4453, -0.8466, -0.2637,  0.2882, -0.7683, -1.8714,\n",
      "          -0.5708],\n",
      "         [ 0.4236,  0.7944, -0.3965,  2.0837,  1.5313, -0.2555, -0.5523,\n",
      "           0.2299],\n",
      "         [-0.2144, -2.1346, -1.1441,  0.1578,  1.0576, -0.3471, -0.7582,\n",
      "          -0.6512]],\n",
      "\n",
      "        [[-2.1511,  0.2545, -0.3785, -0.8013, -2.2646, -0.7982,  0.1091,\n",
      "          -0.0393],\n",
      "         [ 1.2741,  0.0379,  1.8489,  0.5550, -0.9414, -0.5049, -2.2548,\n",
      "           2.4859],\n",
      "         [-0.9429, -1.4084, -2.9085, -0.2017,  0.5564, -0.3930,  0.8198,\n",
      "          -0.7483]],\n",
      "\n",
      "        [[-1.8498,  0.5723,  2.2822, -0.0952,  0.6893,  0.2898,  0.4487,\n",
      "           1.5775],\n",
      "         [-1.2987,  0.7168, -0.1050,  1.4360,  2.0061, -0.6024, -0.5367,\n",
      "           1.1440],\n",
      "         [-0.4555,  1.7813,  0.3584,  1.0669,  0.3544,  0.3956, -0.0149,\n",
      "           0.6530]],\n",
      "\n",
      "        [[-1.1172, -0.6415,  0.2045,  0.3716,  0.7389, -0.3432, -0.3856,\n",
      "          -0.7825],\n",
      "         [ 1.0252,  1.2965,  1.1913, -0.5153,  0.6247, -0.9956, -0.4653,\n",
      "          -0.7261],\n",
      "         [-0.5415,  0.2010,  0.5491, -0.0056, -0.1048,  1.1279, -0.1295,\n",
      "           0.9701]],\n",
      "\n",
      "        [[ 0.5706, -1.7227,  1.6322, -1.7856,  0.6109,  0.8977, -0.4624,\n",
      "          -1.5706],\n",
      "         [-0.9701,  0.7955,  2.1552,  1.3327,  2.1070, -0.8306, -0.1698,\n",
      "           1.6310],\n",
      "         [ 0.2176, -0.1083, -2.3262,  2.9421, -1.1652,  1.3394,  1.1004,\n",
      "           0.6377]]])\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T14:26:54.827361Z",
     "start_time": "2025-07-27T14:26:54.802319Z"
    }
   },
   "cell_type": "code",
   "source": "layer_norm_layer = LayerNormalization(inputs.size()[-2:])",
   "id": "7cf6791e8ad175c5",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T14:27:09.019190Z",
     "start_time": "2025-07-27T14:27:08.977260Z"
    }
   },
   "cell_type": "code",
   "source": "out = layer_norm_layer.forward(inputs)",
   "id": "ec6b3cf60ce47664",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean.size()= torch.Size([5, 1, 1])\n",
      " mean= tensor([[[-0.1733]],\n",
      "\n",
      "        [[-0.3665]],\n",
      "\n",
      "        [[ 0.4506]],\n",
      "\n",
      "        [[ 0.0644]],\n",
      "\n",
      "        [[ 0.2858]]])\n",
      "Standard Deviation, size= torch.Size([5, 1, 1])\n",
      " std= tensor([[[0.9393]],\n",
      "\n",
      "        [[1.2724]],\n",
      "\n",
      "        [[0.9780]],\n",
      "\n",
      "        [[0.7241]],\n",
      "\n",
      "        [[1.3807]]])\n",
      "y.size()= torch.Size([5, 3, 8])\n",
      " y= tensor([[[-2.3723e-01,  6.5859e-01, -7.1683e-01, -9.6298e-02,  4.9133e-01,\n",
      "          -6.3350e-01, -1.8079e+00, -4.2318e-01],\n",
      "         [ 6.3544e-01,  1.0302e+00, -2.3767e-01,  2.4029e+00,  1.8148e+00,\n",
      "          -8.7517e-02, -4.0348e-01,  4.2925e-01],\n",
      "         [-4.3778e-02, -2.0881e+00, -1.0335e+00,  3.5253e-01,  1.3104e+00,\n",
      "          -1.8501e-01, -6.2266e-01, -5.0881e-01]],\n",
      "\n",
      "        [[-1.4025e+00,  4.8806e-01, -9.4283e-03, -3.4170e-01, -1.4917e+00,\n",
      "          -3.3929e-01,  3.7372e-01,  2.5715e-01],\n",
      "         [ 1.2893e+00,  3.1777e-01,  1.7410e+00,  7.2417e-01, -4.5182e-01,\n",
      "          -1.0882e-01, -1.4840e+00,  2.2417e+00],\n",
      "         [-4.5303e-01, -8.1885e-01, -1.9977e+00,  1.2948e-01,  7.2530e-01,\n",
      "          -2.0873e-02,  9.3227e-01, -3.0006e-01]],\n",
      "\n",
      "        [[-2.3521e+00,  1.2442e-01,  1.8727e+00, -5.5809e-01,  2.4407e-01,\n",
      "          -1.6438e-01, -1.9644e-03,  1.1523e+00],\n",
      "         [-1.7886e+00,  2.7217e-01, -5.6805e-01,  1.0076e+00,  1.5904e+00,\n",
      "          -1.0766e+00, -1.0094e+00,  7.0903e-01],\n",
      "         [-9.2649e-01,  1.3606e+00, -9.4220e-02,  6.3015e-01, -9.8308e-02,\n",
      "          -5.6264e-02, -4.7591e-01,  2.0697e-01]],\n",
      "\n",
      "        [[-1.6320e+00, -9.7490e-01,  1.9345e-01,  4.2413e-01,  9.3148e-01,\n",
      "          -5.6301e-01, -6.2160e-01, -1.1696e+00],\n",
      "         [ 1.3269e+00,  1.7016e+00,  1.5562e+00, -8.0072e-01,  7.7375e-01,\n",
      "          -1.4641e+00, -7.3165e-01, -1.0918e+00],\n",
      "         [-8.3688e-01,  1.8861e-01,  6.6929e-01, -9.6811e-02, -2.3381e-01,\n",
      "           1.4686e+00, -2.6786e-01,  1.2507e+00]],\n",
      "\n",
      "        [[ 2.0627e-01, -1.4546e+00,  9.7513e-01, -1.5002e+00,  2.3548e-01,\n",
      "           4.4321e-01, -5.4185e-01, -1.3445e+00],\n",
      "         [-9.0960e-01,  3.6917e-01,  1.3539e+00,  7.5824e-01,  1.3191e+00,\n",
      "          -8.0850e-01, -3.2992e-01,  9.7431e-01],\n",
      "         [-4.9397e-02, -2.8544e-01, -1.8918e+00,  1.9238e+00, -1.0508e+00,\n",
      "           7.6310e-01,  5.9001e-01,  2.5487e-01]]])\n",
      "out.size()= torch.Size([5, 3, 8])\n",
      " out= tensor([[[-2.3723e-01,  6.5859e-01, -7.1683e-01, -9.6298e-02,  4.9133e-01,\n",
      "          -6.3350e-01, -1.8079e+00, -4.2318e-01],\n",
      "         [ 6.3544e-01,  1.0302e+00, -2.3767e-01,  2.4029e+00,  1.8148e+00,\n",
      "          -8.7517e-02, -4.0348e-01,  4.2925e-01],\n",
      "         [-4.3778e-02, -2.0881e+00, -1.0335e+00,  3.5253e-01,  1.3104e+00,\n",
      "          -1.8501e-01, -6.2266e-01, -5.0881e-01]],\n",
      "\n",
      "        [[-1.4025e+00,  4.8806e-01, -9.4283e-03, -3.4170e-01, -1.4917e+00,\n",
      "          -3.3929e-01,  3.7372e-01,  2.5715e-01],\n",
      "         [ 1.2893e+00,  3.1777e-01,  1.7410e+00,  7.2417e-01, -4.5182e-01,\n",
      "          -1.0882e-01, -1.4840e+00,  2.2417e+00],\n",
      "         [-4.5303e-01, -8.1885e-01, -1.9977e+00,  1.2948e-01,  7.2530e-01,\n",
      "          -2.0873e-02,  9.3227e-01, -3.0006e-01]],\n",
      "\n",
      "        [[-2.3521e+00,  1.2442e-01,  1.8727e+00, -5.5809e-01,  2.4407e-01,\n",
      "          -1.6438e-01, -1.9644e-03,  1.1523e+00],\n",
      "         [-1.7886e+00,  2.7217e-01, -5.6805e-01,  1.0076e+00,  1.5904e+00,\n",
      "          -1.0766e+00, -1.0094e+00,  7.0903e-01],\n",
      "         [-9.2649e-01,  1.3606e+00, -9.4220e-02,  6.3015e-01, -9.8308e-02,\n",
      "          -5.6264e-02, -4.7591e-01,  2.0697e-01]],\n",
      "\n",
      "        [[-1.6320e+00, -9.7490e-01,  1.9345e-01,  4.2413e-01,  9.3148e-01,\n",
      "          -5.6301e-01, -6.2160e-01, -1.1696e+00],\n",
      "         [ 1.3269e+00,  1.7016e+00,  1.5562e+00, -8.0072e-01,  7.7375e-01,\n",
      "          -1.4641e+00, -7.3165e-01, -1.0918e+00],\n",
      "         [-8.3688e-01,  1.8861e-01,  6.6929e-01, -9.6811e-02, -2.3381e-01,\n",
      "           1.4686e+00, -2.6786e-01,  1.2507e+00]],\n",
      "\n",
      "        [[ 2.0627e-01, -1.4546e+00,  9.7513e-01, -1.5002e+00,  2.3548e-01,\n",
      "           4.4321e-01, -5.4185e-01, -1.3445e+00],\n",
      "         [-9.0960e-01,  3.6917e-01,  1.3539e+00,  7.5824e-01,  1.3191e+00,\n",
      "          -8.0850e-01, -3.2992e-01,  9.7431e-01],\n",
      "         [-4.9397e-02, -2.8544e-01, -1.8918e+00,  1.9238e+00, -1.0508e+00,\n",
      "           7.6310e-01,  5.9001e-01,  2.5487e-01]]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "bf0344fe3aed34a2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
